{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rywua934-Qyh"
      },
      "source": [
        "# Load gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2ngVfRaqGDC",
        "outputId": "0778d3d4-9950-41c2-9f1d-ec6242a892f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ekMcHmWWqH4s"
      },
      "outputs": [],
      "source": [
        "root_dir=\"/content/drive/MyDrive/Semester-Docs/Sem5/Capstone/\"\n",
        "proj_dir=root_dir+\"rsna-pneumonia-detection-challenge/\"\n",
        "rsna_train_dir=proj_dir+\"stage_2_train_images/\"\n",
        "rsna_metadata_path=proj_dir+\"stage_2_train_labels.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "24e0Y5Xg1hZ5"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "# dataset = 'rsa_pneumonia.pkl_final'\n",
        "dataset1 = 'chest_xray_pneumonia.pkl_final'\n",
        "dataset2 = 'chest_xray_augment.pkl_final'\n",
        "pkl_file_name = 'chest_xray_pneumonia_augment.pkl_final'\n",
        "\n",
        "with open(proj_dir + dataset1, 'rb') as file:\n",
        "  merged_df = pickle.load(file)\n",
        "with open(proj_dir + dataset2, 'rb') as file:\n",
        "  augment_df = pickle.load(file)\n",
        "merged_df = pd.concat([merged_df, augment_df], ignore_index=True)\n",
        "augment_df.to_pickle(proj_dir+pkl_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Augmentor"
      ],
      "metadata": {
        "id": "OvM39whNh3gD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z57K6ZY043DH",
        "outputId": "faab60e2-6c28-49ee-c88f-2a6d22bc68f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Augmentor in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (9.4.0)\n",
            "Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (4.66.4)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install Augmentor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stgYRgBV42Ig",
        "outputId": "ae24da7c-1f44-44f7-abd3-51d9d01701bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 3875 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Semester-Docs/Sem5/Capstone/chest_xray/augment/PNEUMONIA."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=L size=1160x800 at 0x79B43950FBB0>: 100%|██████████| 5812/5812 [07:51<00:00, 12.34 Samples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data augmentation completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import Augmentor\n",
        "\n",
        "source_data_dir = '/content/drive/MyDrive/Semester-Docs/Sem5/Capstone/chest_xray/train/PNEUMONIA/'\n",
        "output_data_dir = '/content/drive/MyDrive/Semester-Docs/Sem5/Capstone/chest_xray/augment/PNEUMONIA'\n",
        "# output_data_dir = '/content/drive/MyDrive/Semester-Docs/Sem5/Capstone/rsna-pneumonia-detection-challenge/augment/PNEUMONIA'\n",
        "os.makedirs(output_data_dir, exist_ok=True)\n",
        "pipeline = Augmentor.Pipeline(source_directory=source_data_dir, output_directory=output_data_dir)\n",
        "\n",
        "pipeline.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "pipeline.flip_left_right(probability=0.5)\n",
        "pipeline.flip_top_bottom(probability=0.5)\n",
        "pipeline.zoom_random(probability=0.5, percentage_area=0.8)\n",
        "pipeline.flip_random(probability=0.5)\n",
        "pipeline.rotate_random_90(probability=0.5)\n",
        "pipeline.random_contrast(probability=0.5, min_factor=0.5, max_factor=1.5)\n",
        "\n",
        "num_augmented_images = (int)(len(to_be_augment_pos_image_paths) * 1.5)\n",
        "\n",
        "pipeline.sample(num_augmented_images)\n",
        "print(\"Data augmentation completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DF from augment data"
      ],
      "metadata": {
        "id": "Z-9tiMcSwlkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "output_data_dir = '/content/drive/MyDrive/Semester-Docs/Sem5/Capstone/chest_xray/augment'\n",
        "def create_file_map_all(root_dir, limit=100):\n",
        "    file_map_all = {0:{},1:{}} # Initialize an empty map\n",
        "    contents = os.listdir(root_dir)\n",
        "    directories = [entry for entry in contents if os.path.isdir(os.path.join(root_dir, entry))]\n",
        "    if (len(directories) != 2):\n",
        "      print(\"Please check the directories must be 2 and it should be ['NORMAL', 'PNEUMONIA']\")\n",
        "      return;\n",
        "\n",
        "    for directory in directories:\n",
        "      if directory == 'NORMAL':\n",
        "        label=0\n",
        "      elif directory == 'PNEUMONIA':\n",
        "        label=1\n",
        "      else:\n",
        "        print(\"Directory should be ['NORMAL', 'PNEUMONIA']\")\n",
        "        return;\n",
        "      new_root_dir = Path(root_dir + \"/\" + directory)\n",
        "      files = [file for file in new_root_dir.iterdir() if file.is_file()]\n",
        "      for filename_full_path in files:\n",
        "        filename = Path(filename_full_path).name\n",
        "        full_path = os.path.join(new_root_dir, filename)\n",
        "        if len(file_map_all[label]) != limit:\n",
        "          file_map_all[label][filename] = full_path\n",
        "        else:\n",
        "          break\n",
        "    return file_map_all\n",
        "\n",
        "start_time = time.time()\n",
        "file_map_all = create_file_map_all(output_data_dir, 10)\n",
        "\n",
        "augment_file_map_0 = file_map_all[0]\n",
        "augment_file_map_1 = file_map_all[1]\n",
        "\n",
        "augment_df_0 = pd.DataFrame(augment_file_map_0.items(), columns=['file_name', 'file_path'])\n",
        "augment_label_0 = pd.DataFrame({'label': np.zeros(len(augment_df_0))})\n",
        "augment_df_new_0 = pd.concat([augment_df_0, augment_label_0], axis=1)\n",
        "\n",
        "augment_df_1 = pd.DataFrame(augment_file_map_1.items(), columns=['file_name', 'file_path'])\n",
        "augment_label_1 = pd.DataFrame({'label': np.ones(len(augment_df_1))})\n",
        "augment_df_new_1 = pd.concat([augment_df_1, augment_label_1], axis=1)\n",
        "\n",
        "augment_df = pd.concat([augment_df_new_0, augment_df_new_1], ignore_index=True)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "print(augment_df)"
      ],
      "metadata": {
        "id": "gy5gxw-MwlJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0f9mFkF1UbI"
      },
      "source": [
        "# Loading file and creating a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-7qYBxpi-9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cff3dd-5d8c-4cc1-fc59-7e99950e9213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 138.8557 seconds\n",
            "                                      file_name  \\\n",
            "0      f79893ef-5f7f-4b63-bcbe-694a2bd8bf42.dcm   \n",
            "1      f70553b5-ee4f-45f4-a023-0d696aeedffe.dcm   \n",
            "2      f7b557b1-623e-4466-a60b-ada96f7ba8ff.dcm   \n",
            "3      f79fa94e-8886-425c-95d4-81abfea92aee.dcm   \n",
            "4      f69b3d65-3650-4bd3-8809-a0fc0eac5f2a.dcm   \n",
            "...                                         ...   \n",
            "26679  09b0df7c-90bc-499d-a7c7-5474199da4e6.dcm   \n",
            "26680  0930b0aa-25c2-4624-a3fc-9b60ba0b23f4.dcm   \n",
            "26681  097788d4-cb88-4457-8e71-0ca7a3da2216.dcm   \n",
            "26682  09326eb7-f4cb-4d8f-83c6-8ba7fb8b5ac7.dcm   \n",
            "26683  0909a8b0-09e7-4a78-9bfb-417a61c6723c.dcm   \n",
            "\n",
            "                                               file_path  label  \n",
            "0      /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    0.0  \n",
            "1      /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    0.0  \n",
            "2      /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    0.0  \n",
            "3      /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    0.0  \n",
            "4      /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    0.0  \n",
            "...                                                  ...    ...  \n",
            "26679  /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    1.0  \n",
            "26680  /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    1.0  \n",
            "26681  /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    1.0  \n",
            "26682  /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    1.0  \n",
            "26683  /content/drive/MyDrive/Semester-Docs/Sem5/Caps...    1.0  \n",
            "\n",
            "[26684 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "def create_file_map_all(root_dir, metadata, limit=100):\n",
        "    file_map_all = {0:{},1:{}} # Initialize an empty map\n",
        "    root_dir = Path(root_dir)\n",
        "    files = [file for file in root_dir.iterdir() if file.is_file()]\n",
        "    for filename_full_path in files:\n",
        "        filename = Path(filename_full_path).name\n",
        "        meta_row = metadata[metadata['patientId'] == os.path.splitext(filename)[0]]\n",
        "        if (len(meta_row)) > 0:\n",
        "          meta_row_first = meta_row[0]\n",
        "          label = meta_row_first[5]\n",
        "          full_path = os.path.join(root_dir, filename)\n",
        "          if len(file_map_all[label]) != limit:\n",
        "            file_map_all[label][filename] = full_path\n",
        "    return file_map_all\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "metadata = np.genfromtxt(rsna_metadata_path, delimiter=',', names=True,\n",
        "        dtype='U40,i4,i4,i4,i4,i4')\n",
        "\n",
        "file_map_all = create_file_map_all(rsna_train_dir, metadata, 100000)\n",
        "\n",
        "mrnet_file_map = file_map_all[0]\n",
        "knee_mri_files_map = file_map_all[1]\n",
        "\n",
        "mrnet_df = pd.DataFrame(mrnet_file_map.items(), columns=['file_name', 'file_path'])\n",
        "mrnet_label = pd.DataFrame({'label': np.zeros(len(mrnet_df))})\n",
        "mrnet_df_new = pd.concat([mrnet_df, mrnet_label], axis=1)\n",
        "\n",
        "knee_mri_df = pd.DataFrame(knee_mri_files_map.items(), columns=['file_name', 'file_path'])\n",
        "knee_mri_label = pd.DataFrame({'label': np.ones(len(knee_mri_df))})\n",
        "knee_mri_df_new = pd.concat([knee_mri_df, knee_mri_label], axis=1)\n",
        "\n",
        "merged_df = pd.concat([mrnet_df_new, knee_mri_df_new], ignore_index=True)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
        "\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBG9zy5k-VKB"
      },
      "source": [
        "# Download all tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppOkff9wD6di"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/subrat-mishra/MedSam-Utils\n",
        "# import sys\n",
        "# sys.path.append(root_dir)\n",
        "# sys.path.append(\"MedSam-Utils\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkeEVB27D0i9",
        "outputId": "8d1a1812-950c-41a0-97bd-1764531dffc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "Collecting git+https://github.com/bowang-lab/MedSAM.git\n",
            "  Cloning https://github.com/bowang-lab/MedSAM.git to /tmp/pip-req-build-bc9aswx7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/bowang-lab/MedSAM.git /tmp/pip-req-build-bc9aswx7\n",
            "  Resolved https://github.com/bowang-lab/MedSAM.git to commit 91ce9665656c4420b3781f5de87fc4daa7e596a0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting monai (from medsam==0.0.1)\n",
            "  Downloading monai-1.3.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from medsam==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medsam==0.0.1) (0.19.3)\n",
            "Collecting SimpleITK>=2.2.1 (from medsam==0.0.1)\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from medsam==0.0.1) (4.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medsam==0.0.1) (4.66.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from medsam==0.0.1) (1.11.4)\n",
            "Collecting ipympl (from medsam==0.0.1)\n",
            "  Downloading ipympl-0.9.4-py3-none-any.whl (516 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from medsam==0.0.1) (4.8.0.76)\n",
            "Collecting jupyterlab (from medsam==0.0.1)\n",
            "  Downloading jupyterlab-4.2.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from medsam==0.0.1) (7.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipympl->medsam==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: ipython<9 in /usr/local/lib/python3.10/dist-packages (from ipympl->medsam==0.0.1) (7.34.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ipympl->medsam==0.0.1) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from ipympl->medsam==0.0.1) (9.4.0)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.10/dist-packages (from ipympl->medsam==0.0.1) (5.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->medsam==0.0.1) (5.5.6)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->medsam==0.0.1) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->medsam==0.0.1) (3.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->medsam==0.0.1) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->medsam==0.0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->medsam==0.0.1) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->medsam==0.0.1) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->medsam==0.0.1) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->medsam==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->medsam==0.0.1) (2.8.2)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->medsam==0.0.1)\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Collecting httpx>=0.25.0 (from jupyterlab->medsam==0.0.1)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipykernel>=4.5.1 (from ipywidgets->medsam==0.0.1)\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->medsam==0.0.1) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from jupyterlab->medsam==0.0.1) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->medsam==0.0.1)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-server<3,>=2.4.0 (from jupyterlab->medsam==0.0.1)\n",
            "  Downloading jupyter_server-2.14.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->medsam==0.0.1)\n",
            "  Downloading jupyterlab_server-2.27.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->medsam==0.0.1) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->medsam==0.0.1) (67.7.2)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->medsam==0.0.1) (2.0.1)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->medsam==0.0.1) (6.3.3)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai->medsam==0.0.1) (2.3.0+cu121)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsam==0.0.1) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsam==0.0.1) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsam==0.0.1) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medsam==0.0.1) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from async-lru>=1.0.0->jupyterlab->medsam==0.0.1) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->medsam==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->medsam==0.0.1) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab->medsam==0.0.1)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->medsam==0.0.1) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->medsam==0.0.1) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->jupyterlab->medsam==0.0.1)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1 (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1) (1.6.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1) (24.0.1)\n",
            "Collecting jedi>=0.16 (from ipython<9->ipympl->medsam==0.0.1)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->medsam==0.0.1) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->medsam==0.0.1) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->medsam==0.0.1) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->medsam==0.0.1) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->medsam==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->medsam==0.0.1) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.3->jupyterlab->medsam==0.0.1) (2.1.5)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->jupyterlab->medsam==0.0.1) (4.2.2)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (23.1.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1)\n",
            "  Downloading jupyter_client-8.6.2-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1)\n",
            "  Downloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (5.10.4)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (0.20.0)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (1.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (2.15.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1)\n",
            "  Downloading json5-0.9.25-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (4.19.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->medsam==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai->medsam==0.0.1) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai->medsam==0.0.1) (1.12.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai->medsam==0.0.1) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.9->monai->medsam==0.0.1)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai->medsam==0.0.1) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->monai->medsam==0.0.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->medsam==0.0.1) (6.5.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->medsam==0.0.1) (1.2.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (21.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<9->ipympl->medsam==0.0.1) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (0.18.1)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1)\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (6.0.1)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (2.19.1)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=4.5.1->ipywidgets->medsam==0.0.1)\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->medsam==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<9->ipympl->medsam==0.0.1) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<9->ipympl->medsam==0.0.1) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai->medsam==0.0.1) (1.3.0)\n",
            "Collecting fqdn (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting uri-template (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1) (24.6.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->medsam==0.0.1) (2.22)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->medsam==0.0.1)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: medsam\n",
            "  Building wheel for medsam (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medsam: filename=medsam-0.0.1-py3-none-any.whl size=37343 sha256=4b3d51290f250f10d247b9407e88f6b869cc9784edfa06f7983e7957ca408969\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k95_03ki/wheels/d8/58/d0/a5f7456e7ae2b4e561cc9e0a065afeb4a9e2b7aa2fbc0fe80c\n",
            "Successfully built medsam\n",
            "Installing collected packages: SimpleITK, uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jsonpointer, json5, jedi, h11, fqdn, comm, async-lru, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jupyter-server-terminals, jupyter-client, httpcore, arrow, nvidia-cusolver-cu12, isoduration, ipykernel, httpx, monai, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, ipympl, medsam\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SimpleITK-2.3.1 arrow-1.3.0 async-lru-2.0.4 comm-0.2.2 fqdn-1.5.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 ipykernel-6.29.4 ipympl-0.9.4 isoduration-20.11.0 jedi-0.19.1 json5-0.9.25 jsonpointer-3.0.0 jupyter-client-7.4.9 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter-server-2.14.1 jupyter-server-terminals-0.5.3 jupyterlab-4.2.2 jupyterlab-server-2.27.2 medsam-0.0.1 monai-1.3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 overrides-7.7.0 python-json-logger-2.0.7 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 types-python-dateutil-2.9.0.20240316 uri-template-1.3.0\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.6\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.4.4\n",
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.5.2.tar.gz (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (9.4.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.18.0+cu121)\n",
            "Collecting ttach (from grad-cam)\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.66.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->grad-cam) (12.5.40)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.2-py3-none-any.whl size=38335 sha256=1635c6d249318568863e1e8cb347ce58118da6f5103ac30dd7471668bf6ba7c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/68/bb/d10381e86dc0de1c9354bce3d86bffcd247305058c40ce2e55\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.5.2 ttach-0.0.3\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorboardX\n",
        "!pip3 install git+https://github.com/bowang-lab/MedSAM.git\n",
        "!pip install umap-learn\n",
        "!pip install pydicom\n",
        "!pip install grad-cam\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANAbi7CAD0BT",
        "outputId": "31d4a5e0-9004-4689-bd27-41f63b2c256f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Qf0IQF1zTwOJ4Fh4Ix5rtVhR9cnNbI2M\n",
            "To: /content/img_demo.png\n",
            "100% 87.9k/87.9k [00:00<00:00, 29.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1UAmWL88roYR7wKlnApw5Bcuzf2iQgk6_\n",
            "From (redirected): https://drive.google.com/uc?id=1UAmWL88roYR7wKlnApw5Bcuzf2iQgk6_&confirm=t&uuid=7db54240-cd78-413c-9dff-d7d29b695799\n",
            "To: /content/medsam_vit_b.pth\n",
            "100% 375M/375M [00:03<00:00, 108MB/s]\n"
          ]
        }
      ],
      "source": [
        "# download model and data\n",
        "img_id = '1Qf0IQF1zTwOJ4Fh4Ix5rtVhR9cnNbI2M'\n",
        "!gdown $img_id\n",
        "model_id = \"1UAmWL88roYR7wKlnApw5Bcuzf2iQgk6_&confirm=t\"\n",
        "!gdown $model_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTscYE66-cO0"
      },
      "source": [
        "# MedSam Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_MTE96mKDpBa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from segment_anything import sam_model_registry\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MedSAM_CKPT_PATH = \"medsam_vit_b.pth\"\n",
        "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
        "medsam_model = medsam_model.to(device)\n",
        "medsam_model.eval()\n",
        "\n",
        "def get_medsam_embeddings(img_3c):\n",
        "  img_1024 = transform.resize(img_3c, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n",
        "  img_1024 = (img_1024 - img_1024.min()) / np.clip(\n",
        "      img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n",
        "  )\n",
        "  # convert the shape to (3, H, W)\n",
        "  img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      image_embedding = medsam_model.image_encoder(img_1024_tensor) # (1, 256, 64, 64)\n",
        "  return image_embedding\n",
        "\n",
        "def get_knee_mri_embedding(file_path):\n",
        "  metadata_csv_path = root_dir + \"KneeMRI/metadata.csv\"\n",
        "  metadata = np.genfromtxt(metadata_csv_path, delimiter=',', names=True,\n",
        "        dtype='i4,i4,i4,i4,i4,i4,i4,i4,i4,i4,U20')\n",
        "  file_name = os.path.basename(file_path)\n",
        "  exam = metadata[metadata['volumeFilename'] == file_name]\n",
        "  with open(file_path, 'rb') as file_handler: # Must use 'rb' as the data is binary\n",
        "      volumetric_data = pickle.load(file_handler)\n",
        "  z_start = exam['roiZ']\n",
        "  depth = exam['roiDepth']\n",
        "  mid_depth = depth if depth == 1 else (int)(depth/2)\n",
        "  img_np = volumetric_data[z_start + mid_depth, :, :]\n",
        "  img_np = img_np.reshape(-1, img_np.shape[-1])\n",
        "  return img_np\n",
        "\n",
        "def get_image_arr(file_path):\n",
        "  if file_path.endswith(\"npy\"):\n",
        "    np_load = np.load(file_path)\n",
        "    mid = (int)(np_load.shape[0]/2) - 1\n",
        "    img_np = np_load[mid] # Mid slice\n",
        "  elif file_path.endswith(\"pck\"):\n",
        "    # Special case for KneeMRI image\n",
        "    img_np = get_knee_mri_embedding(file_path)\n",
        "  elif file_path.endswith(\"dcm\"):\n",
        "    img_np = pydicom.dcmread(file_path).pixel_array\n",
        "  else:\n",
        "    image = Image.open(file_path)\n",
        "    img_np = np.array(image)\n",
        "\n",
        "  return img_np\n",
        "\n",
        "def get_output_tensor(file_path):\n",
        "  img_np = get_image_arr(file_path)\n",
        "  if len(img_np.shape) == 2:\n",
        "      img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n",
        "  else:\n",
        "      img_3c = img_np\n",
        "  embeddings = get_medsam_embeddings(img_3c)\n",
        "  global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "  output_tensor = global_avg_pool(embeddings)\n",
        "  output_tensor = output_tensor.view(embeddings.size(1))\n",
        "  return output_tensor.cpu().numpy()\n",
        "\n",
        "def get_output_tensor_from_np(img_np):\n",
        "  if len(img_np.shape) == 2:\n",
        "      img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n",
        "  else:\n",
        "      img_3c = img_np\n",
        "  embeddings = get_medsam_embeddings(img_3c)\n",
        "  global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "  output_tensor = global_avg_pool(embeddings)\n",
        "  output_tensor = output_tensor.view(embeddings.size(1))\n",
        "  return output_tensor.cpu().numpy()\n",
        "\n",
        "def compute_embedding(index, total, file_path, start_time):\n",
        "  output_tensor_arr = get_output_tensor(file_path)\n",
        "  if index % 10 == 0:\n",
        "    print(f\"Time taken: {index}/{total} is {time.time() - start_time:.4f} seconds\")\n",
        "  return output_tensor_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNnBTbP5f8L7"
      },
      "source": [
        "# Update dataframe with embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B364ciKf5OY"
      },
      "outputs": [],
      "source": [
        "# start_time = time.time()\n",
        "\n",
        "# merged_df_copy = merged_df.copy()\n",
        "# batch_size = 100\n",
        "# file_batch_size = batch_size\n",
        "# start = 0\n",
        "# for i in range(start, len(merged_df_copy), batch_size):\n",
        "#   merged_df_copy.loc[i:i+batch_size, 'embeddings'] = merged_df_copy.loc[i:i+batch_size].apply(lambda row: compute_embedding(row.name, len(merged_df), row['file_path'], start_time), axis=1)\n",
        "\n",
        "#   if (i + batch_size) % file_batch_size == 0:\n",
        "#     pkl_file_name = 'rsa_pneumonia.pkl_'+str(i + batch_size)\n",
        "#     print(\"Writing file=\", pkl_file_name)\n",
        "#     merged_df_copy.to_pickle(proj_dir+pkl_file_name)\n",
        "\n",
        "# pkl_file_name = 'rsa_pneumonia.pkl_final'\n",
        "# print(\"Writing file=\", pkl_file_name)\n",
        "# merged_df_copy.to_pickle(proj_dir+pkl_file_name)\n",
        "\n",
        "# merged_df = merged_df_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4NALdkbSCir"
      },
      "source": [
        "# TSNE graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBXSOH7URHTL"
      },
      "outputs": [],
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import plotly.express as px\n",
        "# import pandas as pd\n",
        "\n",
        "# file_names = merged_df['file_name']\n",
        "# embeddings = merged_df['embeddings']\n",
        "# labels = merged_df['label'].to_numpy()\n",
        "# labels = ['Tear' if elem > 0 else 'No Tear' for elem in labels]\n",
        "\n",
        "# embeddings = np.stack(embeddings)\n",
        "\n",
        "# tsne = TSNE(n_components=2)\n",
        "\n",
        "# # Fit the t-SNE model to the embedding feature\n",
        "# tsne_results = tsne.fit_transform(embeddings)\n",
        "\n",
        "# df = pd.DataFrame({'x_axis': tsne_results[:, 0], 'y_axis': tsne_results[:, 1],\n",
        "#                    'labels':labels, 'file_names': file_names},\n",
        "#                   columns=['x_axis', 'y_axis', 'labels', 'file_names'])\n",
        "# fig = px.scatter(df, x=\"x_axis\", y=\"y_axis\", color=\"labels\", hover_name=\"file_names\")\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8ZXUlru-KZH"
      },
      "source": [
        "## Debug Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEA4ERbKq8HE"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# import os\n",
        "# import re\n",
        "# import time\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.patches as patch\n",
        "\n",
        "# all_files_map = {}\n",
        "# all_files_map.update(mrnet_file_map)\n",
        "# all_files_map.update(knee_mri_files_map)\n",
        "\n",
        "# view_images = [\"0057.npy\",\"483367-7.pck\"]\n",
        "\n",
        "# def plot_img(img_3c):\n",
        "#   image_path = 'output_image.png'\n",
        "#   plt.imshow(img_3c, cmap='gray')\n",
        "#   # plt.axis('off')\n",
        "#   plt.savefig(image_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "#   plt.show()\n",
        "\n",
        "# # path to metadata csv file\n",
        "# for each_view in view_images:\n",
        "#   file_path = all_files_map.get(each_view)\n",
        "#   img_3c = get_image_arr(file_path)\n",
        "#   print(img_3c.shape)\n",
        "#   plot_img(img_3c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDmi_aNpGQQt"
      },
      "source": [
        "# Create a simpleNN model with 3 FC layers\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D-GHBlnxGPSg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "\n",
        "class OutputStrategy(Enum):\n",
        "  BCE = 1\n",
        "  CONSTRASTIVE = 2\n",
        "  BCE_CONSTRASTIVE = 3\n",
        "  DOMAIN = 4\n",
        "  BCE_DOMAIN = 5\n",
        "  CONSTRASTIVE_DOMAIN = 6\n",
        "  BCE_CONSTRASTIVE_DOMAIN = 7\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, alpha):\n",
        "    ctx.alpha = alpha\n",
        "    return x.view_as(x)\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    output = grad_output.neg() * ctx.alpha\n",
        "    return output, None\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self, input_size, output_strategy, embeddingEnabled=True):\n",
        "    super(SimpleNN, self).__init__()\n",
        "    self.output_strategy = output_strategy\n",
        "    self.embeddingEnabled = embeddingEnabled\n",
        "\n",
        "    self.class_classifier = nn.Sequential()\n",
        "    self.class_classifier.add_module('c_fc1', nn.Linear(input_size, 128))\n",
        "    self.class_classifier.add_module('c_relu1', nn.ReLU())\n",
        "    self.class_classifier.add_module('c_fc2', nn.Linear(128, 64))\n",
        "    self.class_classifier.add_module('c_relu2', nn.ReLU())\n",
        "    self.class_classifier.add_module('c_fc3', nn.Linear(64, 1))\n",
        "    self.class_classifier.add_module('c_sigmoid', nn.Sigmoid())\n",
        "\n",
        "    if OutputStrategy.DOMAIN.name in output_strategy.name:\n",
        "      self.domain_classifier = nn.Sequential()\n",
        "      self.domain_classifier.add_module('d_fc1', nn.Linear(input_size, 128))\n",
        "      self.domain_classifier.add_module('d_relu1', nn.ReLU())\n",
        "      self.domain_classifier.add_module('d_fc2', nn.Linear(128, 1))\n",
        "      self.domain_classifier.add_module('d_softmax', nn.Sigmoid())\n",
        "\n",
        "  def forward(self, x, y=None, alpha=None):\n",
        "    # if not self.embeddingEnabled:\n",
        "    #   x = get_output_tensor_from_np(x)\n",
        "    output1, output2, output3 = None, None, None\n",
        "    if (OutputStrategy.CONSTRASTIVE.name in output_strategy.name):\n",
        "      output1 = self.class_classifier(x)\n",
        "      output2 = self.class_classifier(y)\n",
        "    elif (OutputStrategy.BCE.name in output_strategy.name):\n",
        "      output1 = self.class_classifier(x)\n",
        "\n",
        "    if alpha is not None and OutputStrategy.DOMAIN.name in output_strategy.name:\n",
        "      reverse_x = ReverseLayerF.apply(x, alpha)\n",
        "      output3 = self.domain_classifier(reverse_x)\n",
        "    return output1, output2, output3\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataframe, is_new_df_req=True):\n",
        "    self.data = dataframe\n",
        "    self.is_new_df_req = is_new_df_req\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    filepath = self.data.iloc[idx]['file_path']\n",
        "    label = torch.tensor(self.data.iloc[idx]['label'], dtype=torch.float32)\n",
        "    embeddings = torch.from_numpy(self.data.iloc[idx]['embeddings'])\n",
        "    if not self.is_new_df_req:\n",
        "      return embeddings, label, filepath, embeddings, label, filepath\n",
        "\n",
        "    # if any(embeddings.isna()):\n",
        "    #   embeddings = get_output_tensor(self.data.iloc[idx]['file_path'])\n",
        "\n",
        "    ## CREATE A NEW DATAFRAME TO AVOID RE-SELECTING ORIGINAL IMAGE\n",
        "    new_dataframe = self.data\n",
        "    length = len(new_dataframe)\n",
        "    target = np.random.randint(0, 2) # Create a random index\n",
        "    random = np.random\n",
        "    if target == 1:\n",
        "      ## GET NEGATIVE COUNTERPART\n",
        "      label_other = label\n",
        "      while label_other == label:\n",
        "        choice = random.choice(length)\n",
        "        if (choice != idx):  # Ignore the same index\n",
        "          label_other = new_dataframe.iloc[choice]['label']\n",
        "    else:\n",
        "      ## GET POSITIVE COUNTERPART\n",
        "      label_other = 1.0 - label\n",
        "      while label_other != label:\n",
        "        choice = random.choice(length)\n",
        "        if (choice != idx):   # Ignore the same index\n",
        "          label_other = new_dataframe.iloc[choice]['label']\n",
        "\n",
        "    # print(f\"Target: {target} length: {length} choice: {choice} otherChoice: {random.choice(length)} label: {label} label_other: {label_other}\")\n",
        "\n",
        "    filepath_ = new_dataframe.iloc[choice]['file_path']\n",
        "    embeddings_ = torch.from_numpy(new_dataframe.iloc[choice]['embeddings'])\n",
        "    label_ = torch.tensor(target, dtype=torch.float32)\n",
        "    return embeddings, label, filepath, embeddings_, label_, filepath_\n",
        "\n",
        "def evaluate_model(model, loader, threshold, output_strategy):\n",
        "  model.eval()\n",
        "  all_predictions, all_labels, file_paths, other_file_paths = [], [], [], []\n",
        "  with torch.no_grad():\n",
        "    for inputs1, labels1, file_path1, inputs2, labels2, file_path2 in loader:\n",
        "      outputs1, outputs2, outputs3 = model(inputs1, inputs2)\n",
        "      if (OutputStrategy.CONSTRASTIVE.name in output_strategy.name):\n",
        "        euclidean_distance = F.pairwise_distance(outputs1, outputs2)\n",
        "        predictions = (euclidean_distance > threshold).float()\n",
        "        all_labels.extend(labels2.cpu().numpy())\n",
        "      elif(OutputStrategy.BCE.name in output_strategy.name):\n",
        "        predictions = (outputs1 > threshold).float()\n",
        "        all_labels.extend(labels1.cpu().numpy())\n",
        "      all_predictions.extend(predictions.cpu().numpy())\n",
        "      file_paths.extend(file_path1)\n",
        "      other_file_paths.extend(file_path2)\n",
        "  model.train()\n",
        "  return all_predictions, all_labels, file_paths, other_file_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ6ME6J5J-tk"
      },
      "source": [
        "# Different types of Contrastive loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7PHYblR1boYW"
      },
      "outputs": [],
      "source": [
        "class MarginContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(MarginContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, target):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
        "        loss_contrastive = torch.mean((1 - target) * torch.pow(euclidean_distance, 2) +\n",
        "                                      (target) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive\n",
        "\n",
        "class NormalizedContrastiveLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NormalizedContrastiveLoss, self).__init__()\n",
        "\n",
        "    def forward(self, output1, output2, target):\n",
        "        output1_normalized = output1 / output1.norm(dim=1, keepdim=True)\n",
        "        output2_normalized = output2 / output2.norm(dim=1, keepdim=True)\n",
        "        cosine_similarity = torch.sum(output1_normalized * output2_normalized, dim=1)\n",
        "        loss_contrastive = torch.mean((1 - target) * (1 - cosine_similarity) +\n",
        "                                       target * torch.clamp(cosine_similarity - 0.5, min=0.0))\n",
        "        return loss_contrastive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDZOjZB_9Aon"
      },
      "source": [
        "# Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0u9EzgGscc5V"
      },
      "outputs": [],
      "source": [
        "# Step 1.0: Declare all parameters\n",
        "dataset1='rsa_pneumonia.pkl_final'\n",
        "dataset2='chest_xray_pneumonia_augment.pkl_final'\n",
        "learning_rate = 0.001\n",
        "best_val_accuracy = 0.0\n",
        "best_model_params = None\n",
        "best_f1 = 0.0\n",
        "best_epoch_id=0\n",
        "threshold = 0.4\n",
        "num_epochs = 100\n",
        "output_strategy = OutputStrategy.BCE_DOMAIN\n",
        "batch_size = 32\n",
        "saved_model_name=output_strategy.name.lower() + \"/\" + dataset1.split(\".\")[0] + \"_\" + dataset2.split(\".\")[0]\n",
        "input_size = 256  # Assuming your embeddings are of size 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMixHPBF8_u2",
        "outputId": "f7f0727d-6472-473a-c4df-6848daccf0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] - Old Accuracy: 0.0000, Old F1 Score: 0.0000 | New Accuracy: 0.7722, New F1 Score: 0.0524\n",
            "Epoch [2/100] - Old Accuracy: 0.7722, Old F1 Score: 0.0524 | New Accuracy: 0.8061, New F1 Score: 0.4279\n",
            "Epoch [3/100] - Old Accuracy: 0.8061, Old F1 Score: 0.4279 | New Accuracy: 0.8058, New F1 Score: 0.5179\n",
            "Epoch [4/100] - Old Accuracy: 0.8058, Old F1 Score: 0.5179 | New Accuracy: 0.8046, New F1 Score: 0.5553\n",
            "Epoch [5/100] - Old Accuracy: 0.8046, Old F1 Score: 0.5553 | New Accuracy: 0.7914, New F1 Score: 0.5807\n",
            "Epoch [6/100] - Curr Accuracy: 0.8118, Curr F1 Score: 0.4861\n",
            "Epoch [7/100] - Old Accuracy: 0.7914, Old F1 Score: 0.5807 | New Accuracy: 0.7749, New F1 Score: 0.5990\n",
            "Epoch [8/100] - Curr Accuracy: 0.8085, Curr F1 Score: 0.5528\n",
            "Epoch [9/100] - Curr Accuracy: 0.8049, Curr F1 Score: 0.5908\n",
            "Epoch [10/100] - Curr Accuracy: 0.8138, Curr F1 Score: 0.5341\n",
            "Epoch [11/100] - Curr Accuracy: 0.8106, Curr F1 Score: 0.5473\n",
            "Epoch [12/100] - Curr Accuracy: 0.8109, Curr F1 Score: 0.5615\n",
            "Epoch [13/100] - Curr Accuracy: 0.8126, Curr F1 Score: 0.5304\n",
            "Epoch [14/100] - Curr Accuracy: 0.8088, Curr F1 Score: 0.5747\n",
            "Epoch [15/100] - Old Accuracy: 0.7749, Old F1 Score: 0.5990 | New Accuracy: 0.7758, New F1 Score: 0.6009\n",
            "Epoch [16/100] - Curr Accuracy: 0.7953, Curr F1 Score: 0.5999\n",
            "Epoch [17/100] - Curr Accuracy: 0.8159, Curr F1 Score: 0.5621\n",
            "Epoch [18/100] - Old Accuracy: 0.7758, Old F1 Score: 0.6009 | New Accuracy: 0.7974, New F1 Score: 0.6014\n",
            "Epoch [19/100] - Curr Accuracy: 0.8040, Curr F1 Score: 0.5948\n",
            "Epoch [20/100] - Curr Accuracy: 0.8156, Curr F1 Score: 0.4533\n",
            "Epoch [21/100] - Curr Accuracy: 0.8112, Curr F1 Score: 0.5806\n",
            "Epoch [22/100] - Curr Accuracy: 0.8040, Curr F1 Score: 0.5988\n",
            "Epoch [23/100] - Curr Accuracy: 0.8162, Curr F1 Score: 0.5496\n",
            "Epoch [24/100] - Old Accuracy: 0.7974, Old F1 Score: 0.6014 | New Accuracy: 0.7968, New F1 Score: 0.6067\n",
            "Epoch [25/100] - Curr Accuracy: 0.8079, Curr F1 Score: 0.5951\n",
            "Epoch [26/100] - Curr Accuracy: 0.8168, Curr F1 Score: 0.5311\n",
            "Epoch [27/100] - Curr Accuracy: 0.8168, Curr F1 Score: 0.5543\n",
            "Epoch [28/100] - Curr Accuracy: 0.8180, Curr F1 Score: 0.5254\n",
            "Epoch [29/100] - Old Accuracy: 0.7968, Old F1 Score: 0.6067 | New Accuracy: 0.8010, New F1 Score: 0.6135\n",
            "Epoch [30/100] - Curr Accuracy: 0.7899, Curr F1 Score: 0.6125\n",
            "Epoch [31/100] - Old Accuracy: 0.8010, Old F1 Score: 0.6135 | New Accuracy: 0.7887, New F1 Score: 0.6150\n",
            "Epoch [32/100] - Curr Accuracy: 0.8082, Curr F1 Score: 0.5871\n",
            "Epoch [33/100] - Old Accuracy: 0.7887, Old F1 Score: 0.6150 | New Accuracy: 0.7983, New F1 Score: 0.6183\n",
            "Epoch [34/100] - Curr Accuracy: 0.8174, Curr F1 Score: 0.5860\n",
            "Epoch [35/100] - Curr Accuracy: 0.7995, Curr F1 Score: 0.6144\n",
            "Epoch [36/100] - Curr Accuracy: 0.7998, Curr F1 Score: 0.6152\n",
            "Epoch [37/100] - Curr Accuracy: 0.8073, Curr F1 Score: 0.5933\n",
            "Epoch [38/100] - Curr Accuracy: 0.8210, Curr F1 Score: 0.5677\n",
            "Epoch [39/100] - Curr Accuracy: 0.8204, Curr F1 Score: 0.5339\n",
            "Epoch [40/100] - Curr Accuracy: 0.8180, Curr F1 Score: 0.5788\n",
            "Epoch [41/100] - Curr Accuracy: 0.8016, Curr F1 Score: 0.6169\n",
            "Epoch [42/100] - Curr Accuracy: 0.8201, Curr F1 Score: 0.5349\n",
            "Epoch [43/100] - Curr Accuracy: 0.8153, Curr F1 Score: 0.6036\n",
            "Epoch [44/100] - Curr Accuracy: 0.8201, Curr F1 Score: 0.5320\n",
            "Test Accuracy: 0.8180, Test F1 Score: 0.5021\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load and preprocess the data\n",
        "from itertools import cycle\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "with open(proj_dir + dataset1, 'rb') as file:\n",
        "  df = pickle.load(file)\n",
        "\n",
        "with open(proj_dir + dataset2, 'rb') as file:\n",
        "  target_df = pickle.load(file)\n",
        "\n",
        "# Step 1.1: Encode labels to numerical values (0 and 1)\n",
        "df['label'] = df['label'].astype('category').cat.codes\n",
        "target_df['label'] = target_df['label'].astype('category').cat.codes\n",
        "\n",
        "# Step 1.2: Split the data into train, test, and validation sets\n",
        "train_df, test_val_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df['label'])\n",
        "test_df, val_df = train_test_split(test_val_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Create DataLoader instances for training, validation, and testing\n",
        "train_dataset = CustomDataset(train_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = CustomDataset(val_df)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_dataset = CustomDataset(test_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "target_dataset = CustomDataset(target_df)\n",
        "target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Step 3: Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN(input_size, output_strategy)\n",
        "criterion = nn.BCELoss()\n",
        "contrastive_criterion = MarginContrastiveLoss()\n",
        "domain_criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Step 4: Train the model\n",
        "for epoch in range(num_epochs):\n",
        "  i = -1\n",
        "  for train_input, target_input in zip(train_loader if len(train_df) > len(target_loader) else train_loader, cycle(target_loader) if len(train_df) < len(target_loader) else target_loader):\n",
        "    i += 1\n",
        "    inputs1, labels1, _, inputs2, labels2, _, inputs3, labels3, _, _, _, _ = train_input[0], train_input[1], train_input[2], train_input[3], train_input[4], train_input[5], target_input[0], target_input[1], target_input[2], target_input[3], target_input[4], target_input[5]\n",
        "    if len(inputs1) != len(inputs3):\n",
        "      break\n",
        "  # for inputs1, labels1, _, inputs2, labels2, _ in train_loader:\n",
        "    if (OutputStrategy.DOMAIN.name in output_strategy.name):\n",
        "      p = float(i + epoch * len(train_df)) / num_epochs / len(train_df)\n",
        "      alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "      optimizer.zero_grad()\n",
        "      outputs1, outputs2, outputs3 = model(inputs1, inputs2, alpha)\n",
        "      outputs_1, outputs_2, outputs_3 = model(inputs3, inputs2, alpha)\n",
        "    else:\n",
        "      optimizer.zero_grad()\n",
        "      outputs1, outputs2, outputs3 = model(inputs1, inputs2)\n",
        "\n",
        "    if (output_strategy == OutputStrategy.BCE):\n",
        "      loss = criterion(outputs1, labels1.view(-1, 1))\n",
        "    elif (output_strategy == OutputStrategy.CONSTRASTIVE):\n",
        "      loss = contrastive_criterion(outputs1, outputs2, labels2.view(-1, 1))\n",
        "    elif (output_strategy == OutputStrategy.BCE_CONSTRASTIVE):\n",
        "      loss = criterion(outputs1, labels1.view(-1, 1))\n",
        "      loss += contrastive_criterion(outputs1, outputs2, labels2.view(-1, 1))\n",
        "    elif (output_strategy == OutputStrategy.BCE_DOMAIN):\n",
        "      loss = criterion(outputs1, labels1.view(-1, 1))\n",
        "      loss += domain_criterion(outputs3, torch.zeros(batch_size).float().view(-1, 1))\n",
        "      loss += domain_criterion(outputs_3, torch.ones(batch_size).float().view(-1, 1))\n",
        "    elif (output_strategy == OutputStrategy.CONSTRASTIVE_DOMAIN):\n",
        "      loss = contrastive_criterion(outputs1, outputs2, labels2.view(-1, 1))\n",
        "      loss += domain_criterion(outputs3, torch.zeros(batch_size).float().view(-1, 1))\n",
        "      loss += domain_criterion(outputs_3, torch.ones(batch_size).float().view(-1, 1))\n",
        "    elif (output_strategy == OutputStrategy.BCE_CONSTRASTIVE_DOMAIN):\n",
        "      loss = criterion(outputs1, labels1.view(-1, 1))\n",
        "      loss += contrastive_criterion(outputs1, outputs2, labels2.view(-1, 1))\n",
        "      loss += domain_criterion(outputs3, torch.zeros(batch_size).float().view(-1, 1))\n",
        "      loss += domain_criterion(outputs_3, torch.ones(batch_size).float().view(-1, 1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  val_predictions, val_labels, _, _ = evaluate_model(model, val_loader, threshold, output_strategy)\n",
        "  # Calculate and print accuracy and F1 score\n",
        "  val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "  val_f1 = f1_score(val_labels, val_predictions)\n",
        "\n",
        "  # if val_accuracy > best_val_accuracy:\n",
        "  if val_f1 > best_f1:\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - '\n",
        "        f'Old Accuracy: {best_val_accuracy:.4f}, Old F1 Score: {best_f1:.4f} | '\n",
        "        f'New Accuracy: {val_accuracy:.4f}, New F1 Score: {val_f1:.4f}')\n",
        "    best_val_accuracy = val_accuracy\n",
        "    best_model_params = model.state_dict()\n",
        "    best_f1 = val_f1\n",
        "    best_epoch_id = epoch\n",
        "  else:\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - '\n",
        "        f'Curr Accuracy: {val_accuracy:.4f}, Curr F1 Score: {val_f1:.4f}')\n",
        "\n",
        "  if epoch - 10 > best_epoch_id:\n",
        "    break\n",
        "\n",
        "parent_directory = os.path.dirname(proj_dir + saved_model_name)\n",
        "if not os.path.exists(parent_directory):\n",
        "  os.makedirs(parent_directory)\n",
        "\n",
        "# Save the model with best model params\n",
        "torch.save(best_model_params, proj_dir + saved_model_name)\n",
        "test_predictions, test_labels, _, _ = evaluate_model(model, test_loader, threshold, output_strategy)\n",
        "\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "test_f1 = f1_score(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc3iccQ09En0"
      },
      "source": [
        "# Evaluate the model on a different datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h8IvCBDc-Sdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c86f1b-d3dc-40ff-da98-42a49ab2996b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Other Dataset Accuracy: 0.7502, Other Dataset F1 Score: 0.8457\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "loaded_model = SimpleNN(input_size, output_strategy, embeddingEnabled=False)\n",
        "loaded_model.load_state_dict(torch.load(proj_dir + saved_model_name))\n",
        "loaded_model.eval()\n",
        "\n",
        "dataset2='chest_xray_pneumonia_augment.pkl_final'\n",
        "# dataset2='rsa_pneumonia.pkl_final'\n",
        "\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "with open(proj_dir+dataset2, 'rb') as file:\n",
        "  df2 = pickle.load(file)\n",
        "\n",
        "dataset_2 = CustomDataset(df2)\n",
        "loader2 = DataLoader(dataset_2, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_predictions, val_labels, _, _ = evaluate_model(loaded_model, loader2, threshold, output_strategy)\n",
        "\n",
        "# Calculate and print accuracy and F1 score\n",
        "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "val_f1 = f1_score(val_labels, val_predictions)\n",
        "\n",
        "print(f\"Other Dataset Accuracy: {val_accuracy:.4f}, Other Dataset F1 Score: {val_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GradCam"
      ],
      "metadata": {
        "id": "qohT3BW1Orj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from torchvision import transforms\n",
        "\n",
        "def get_gradcam_heatmap(image_path, embeddings, model, target_layer, target_class):\n",
        "  gradcam = GradCAM(model=model, target_layers=[target_layer])\n",
        "  img_np = get_image_arr(image_path)\n",
        "  image = Image.fromarray(img_np)\n",
        "  transform = transforms.Resize((256, 256))\n",
        "  image_tensor = transform(torch.tensor(img_np).permute(2,0,1)).to(torch.float32)\n",
        "  print(target_class)\n",
        "  heatmap = gradcam(input_tensor=image_tensor, targets=[ClassifierOutputTarget(target_class)])[0:]\n",
        "\n",
        "  return heatmap\n",
        "\n",
        "loaded_model = SimpleNN(input_size, output_strategy)\n",
        "loaded_model.load_state_dict(torch.load(proj_dir + saved_model_name))\n",
        "loaded_model.eval()\n",
        "\n",
        "with open(proj_dir+dataset2, 'rb') as file:\n",
        "  df2 = pickle.load(file)\n",
        "\n",
        "# first=df2.head(1).to_string(index=False)\n",
        "first=df2.iloc[0]\n",
        "image_path=first.file_path\n",
        "target_class=int(first.label)\n",
        "embeddings=first.embeddings\n",
        "target_layer=loaded_model.class_classifier[-1]\n",
        "heatmap = get_gradcam_heatmap(image_path, embeddings, loaded_model, target_layer, target_class)\n",
        "visualization = show_cam_on_image(image_path, heatmap, use_rgb=True)\n",
        "Image.fromarray(visualization)"
      ],
      "metadata": {
        "id": "qaYPy8XpOtrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F40N5RSu83O"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h3uj1b8Tu8X9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "inference_dir=root_dir+\"chest_xray/test/\"\n",
        "file1 = inference_dir+\"NORMAL/IM-0001-0001.jpeg\"\n",
        "file2 = inference_dir+\"PNEUMONIA/person1_virus_6.jpeg\"\n",
        "\n",
        "file1_path = Path(file1)\n",
        "file2_path = Path(file2)\n",
        "\n",
        "data_dict = {\n",
        "    'file_name': [file1_path.name, file2_path.name],\n",
        "    'file_path': [os.path.join('',file1), os.path.join('',file2)],\n",
        "    'label': [0, 1]\n",
        "}\n",
        "\n",
        "inference_df = pd.DataFrame(data_dict)\n",
        "inference_df_file = root_dir + \"inference_df.pkl\"\n",
        "if os.path.exists(inference_df_file):\n",
        "  with open(inference_df_file, 'rb') as f:\n",
        "    inference_df = pickle.load(f)\n",
        "else:\n",
        "  start_time = time.time()\n",
        "  inference_df['embeddings'] = inference_df.apply(lambda row: compute_embedding(row.name, len(inference_df), row['file_path'], start_time), axis=1)\n",
        "  with open(inference_df_file, 'wb') as f:\n",
        "    pickle.dump(inference_df, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT-hXD7Gzkpy"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "dataset1='rsa_pneumonia.pkl_final'\n",
        "dataset2='chest_xray_pneumonia_augment.pkl_final'\n",
        "input_size = 256\n",
        "output_strategy = OutputStrategy.BCE_DOMAIN\n",
        "threshold = 0.4\n",
        "saved_model_name=output_strategy.name.lower() + \"/\" + dataset1.split(\".\")[0] + \"_\" + dataset2.split(\".\")[0]\n",
        "loaded_model = SimpleNN(input_size, output_strategy, embeddingEnabled=False)\n",
        "loaded_model.load_state_dict(torch.load(proj_dir + saved_model_name))\n",
        "loaded_model.eval()\n",
        "\n",
        "inference_dataset = CustomDataset(inference_df, False)\n",
        "inference_loader = DataLoader(inference_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_predictions, val_labels, file_paths, _ = evaluate_model(loaded_model, inference_loader, threshold, output_strategy)\n",
        "for index, _ in enumerate(val_predictions):\n",
        "  img = mpimg.imread(file_paths[index])\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')  # Turn off axis labels\n",
        "  plt.title(f'Expected Label: {val_predictions[index][0]}, Actual Label: {val_labels[index]}')\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Rywua934-Qyh",
        "qohT3BW1Orj6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}